### Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided GFlowNetsAccurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided GFlowNets

Achieving both accuracy and diverse reasoning remains
challenging for Large Language Models (LLMs) in complex domains like mathematics. A key bottleneck is evaluating intermediate reasoning steps to guide generation without costly human annotations. To address this, we first introduce a novel Process Reward Model (PRM) trained automatically using Monte Carlo Tree Search coupled with a similarity-based data augmentation technique, effectively capturing step-level reasoning quality. Leveraging this PRM, we then adapt Generative Flow Networks (GFlowNets) to operate at the reasoning step level. Unlike traditional reinforcement learning focused on maximizing a single reward, GFlowNets naturally sample diverse, high-quality solutions proportional to their rewards, as measured by our PRM. Empirical evaluation shows strong improvements in both accuracy and solution diversity on challenging mathematical benchmarks (e.g., +2.59% absolute accuracy on MATH Level 5 for Llama3.2-3B), with effective generalization to unseen datasets (+9.4% absolute on SAT MATH). Our work demonstrates the potential of PRM-guided, step-level GFlowNets for developing more robust and versatile mathematical reasoning in LLMs.

In this work, we adapt GFlowNets to operate at the reasoning step level by leveraging a modified version of the Sub-TB Loss, originally introduced in [Amortizing intractable inference in large language models](https://arxiv.org/abs/2310.04363), which we further adjusted to function in a step-level context rather than a token-level context, aiming to enhance mathematical reasoning in Large Language Models (LLMs). Traditional reinforcement learning methods typically focus on maximizing a single reward signal, which can lead to limited exploration and solution diversity. In contrast, GFlowNets naturally encourage diverse solution strategies while preserving accuracy, making them particularly well-suited for complex reasoning tasks.

This work highlights the potential of GFlowNets as a powerful tool for developing more robust, diverse, and versatile mathematical reasoning capabilities in LLMs, paving the way for future advancements in AI-driven problem-solving.  For more details about GFlowNets, refer to the foundational [GFlowNets paper](https://arxiv.org/abs/2111.09266).

<p align="center">
  <img src="./images/reward.png" alt="Reward earned during FineTuning" width="49%">
  <img src="./images/loss.png" alt="Loss" width="49%">
</p>
